# Choose one of the following provider modes
LLM_MODE=hosted # or: local


# Hosted API mode (one of these; app prefers OPENROUTER if present)
OPENROUTER_API_KEY=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
OPENROUTER_MODEL=meta-llama/llama-3.1-8b-instruct
# Or TogetherAI / HF Inference (optional)
#TOGETHER_API_KEY=
#TOGETHER_MODEL=meta-llama/Llama-3-8b-Instruct-Turbo
#HF_API_KEY=
#HF_MODEL=HuggingFaceH4/zephyr-7b-beta


# Local mode via Ollama (runs as sidecar in docker-compose)
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=llama3.1:8b


# Embeddings are local (no API cost):
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2


# Basic app settings
APP_TITLE=ARC-RAG
CHROMA_DIR=/app/storage
DATA_DIR=/app/data